{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpi5f-NuuRbg"
   },
   "source": [
    "##  **Some Helper Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDqrxMpLuhLO"
   },
   "source": [
    "### Softmax Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YoOjTJJpt6Nv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax probabilities for a given input matrix.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n",
    "                       - m is the number of samples.\n",
    "                       - n is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Softmax probability matrix of shape (m, n), where\n",
    "                   each row sums to 1 and represents the probability\n",
    "                   distribution over classes.\n",
    "\n",
    "    Notes:\n",
    "    - The input to softmax is typically computed as: z = XW + b.\n",
    "    - Uses numerical stabilization by subtracting the max value per row.\n",
    "    \"\"\"\n",
    "\n",
    "    # Your Code Here.\n",
    "\n",
    "    # This is the normalization process because we want to scale the data from 0-1, when a the maximum\n",
    "    new_z = z - np.max(z, axis=1,keepdims=True) \n",
    "    exp_z = np.exp(new_z)\n",
    "    softmax = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFnMdHJzrUJV"
   },
   "source": [
    "### Softmax Test Case:\n",
    "\n",
    "This test case checks that each row in the resulting softmax probabilities sums to 1, which is the fundamental property of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL5ToHmkrTr-",
    "outputId": "af1aedc1-61d7-415e-9060-2313cbfde13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n",
      "Softmax function passed the test case!\n"
     ]
    }
   ],
   "source": [
    "# Example test case\n",
    "z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n",
    "softmax_output = softmax(z_test)\n",
    "\n",
    "# Verify if the sum of probabilities for each row is 1 using assert\n",
    "row_sums = np.sum(softmax_output, axis=1)\n",
    "\n",
    "print(row_sums)\n",
    "\n",
    "# Assert that the sum of each row is 1\n",
    "assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n",
    "\n",
    "print(\"Softmax function passed the test case!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8qwCbgC1vyHn"
   },
   "outputs": [],
   "source": [
    "def predict_softmax(X, W, b):\n",
    "    \"\"\"\n",
    "    Predict the class labels for a set of samples using the trained softmax model.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c), where c is the number of classes.\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Predicted class labels of shape (n,), where each value is the index of the predicted class.\n",
    "    \"\"\"\n",
    "    z = np.dot(X,W) + b\n",
    "    y_test_probability = softmax(z)\n",
    "\n",
    "    predicted_classes = np.argmax(y_test_probability, axis=1)\n",
    "\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCGDTavVuXZu"
   },
   "source": [
    "### Test Function for Prediction Function:\n",
    "The test function ensures that the predicted class labels have the same number of elements as the input samples, verifying that the model produces a valid output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "musr99YhucQX",
    "outputId": "0e3beead-0770-4f61-e07c-59ba5375fa98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class labels: [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Define test case\n",
    "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
    "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
    "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
    "\n",
    "# Expected Output:\n",
    "# The function should return an array with class labels (0, 1, or 2)\n",
    "\n",
    "y_pred_test = predict_softmax(X_test, W_test, b_test)\n",
    "\n",
    "# Validate output shape\n",
    "assert y_pred_test.shape == (3,), f\"Test failed: Expected shape (3,), got {y_pred_test.shape}\"\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted class labels:\", y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwejxbajvEle"
   },
   "source": [
    "### Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bjqnULCtun_Z"
   },
   "outputs": [],
   "source": [
    "def loss_softmax(y_pred, y):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for a single sample.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): Predicted probabilities of shape (c,) for a single sample,\n",
    "                             where c is the number of classes.\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (c,), where c is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    float: Cross-entropy loss for the given sample.\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon) # epsilon is the way to handle the log(0) value that could be undefined. The clip function then clips teh values from eppsilon to 1-epsilon\n",
    "    loss = -np.sum(y * np.log(y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXdMIV_cz5Fn"
   },
   "source": [
    "## Test case for Loss Function:\n",
    "This test case Compares loss for correct vs. incorrect predictions.\n",
    "*   Expects low loss for correct predictions.\n",
    "*   Expects high loss for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IhRGquu0N9P",
    "outputId": "89cb7e23-270a-46f3-84bc-f359cdcf9973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss (Correct Predictions): 0.4304\n",
      "Cross-Entropy Loss (Incorrect Predictions): 8.9872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define correct predictions (low loss scenario)\n",
    "y_true_correct = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True one-hot labels\n",
    "y_pred_correct = np.array([[0.9, 0.05, 0.05],\n",
    "                           [0.1, 0.85, 0.05],\n",
    "                           [0.05, 0.1, 0.85]])  # High confidence in the correct class\n",
    "\n",
    "# Define incorrect predictions (high loss scenario)\n",
    "y_pred_incorrect = np.array([[0.05, 0.05, 0.9],  # Highly confident in the wrong class\n",
    "                              [0.1, 0.05, 0.85],\n",
    "                              [0.85, 0.1, 0.05]])\n",
    "\n",
    "# Compute loss for both cases\n",
    "loss_correct = loss_softmax(y_pred_correct, y_true_correct)\n",
    "loss_incorrect = loss_softmax(y_pred_incorrect, y_true_correct)\n",
    "\n",
    "# Validate that incorrect predictions lead to a higher loss\n",
    "assert loss_correct < loss_incorrect, f\"Test failed: Expected loss_correct < loss_incorrect, but got {loss_correct:.4f} >= {loss_incorrect:.4f}\"\n",
    "\n",
    "# Print results\n",
    "print(f\"Cross-Entropy Loss (Correct Predictions): {loss_correct:.4f}\")\n",
    "print(f\"Cross-Entropy Loss (Incorrect Predictions): {loss_incorrect:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0d3fm1-vUlY"
   },
   "source": [
    "### Cost Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yaH9_s0svIGJ"
   },
   "outputs": [],
   "source": [
    "def cost_softmax(X, y, W, b):\n",
    "    \"\"\"\n",
    "    Compute the average softmax regression cost (cross-entropy loss) over all samples.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where n is the number of samples and c is the number of classes.\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    float: Average softmax cost (cross-entropy loss) over all samples.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    z = np.dot(X,W)+b\n",
    "    y_pred = softmax(z)\n",
    "    cost = np.sum([loss_softmax(y_pred[i],y[i]) for i in range(n)])\n",
    "    # Return average loss\n",
    "    return cost / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eGyPFJ33tgY"
   },
   "source": [
    "### Test Case for Cost Function:\n",
    "The test case assures that the cost for the incorrect prediction should be higher than for the correct prediction, confirming that the cost function behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIGAxYQt36Sr",
    "outputId": "ea1093d8-a9d3-44b2-dfb4-d172adfe0bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for correct prediction: 0.0006234364133349324\n",
      "Cost for incorrect prediction: 0.29930861359446115\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example 1: Correct Prediction (Closer predictions)\n",
    "X_correct = np.array([[1.0, 0.0], [0.0, 1.0]])  # Feature matrix for correct predictions\n",
    "y_correct = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, matching predictions)\n",
    "W_correct = np.array([[5.0, -2.0], [-3.0, 5.0]])  # Weights for correct prediction\n",
    "b_correct = np.array([0.1, 0.1])  # Bias for correct prediction\n",
    "\n",
    "# Example 2: Incorrect Prediction (Far off predictions)\n",
    "X_incorrect = np.array([[0.1, 0.9], [0.8, 0.2]])  # Feature matrix for incorrect predictions\n",
    "y_incorrect = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, incorrect predictions)\n",
    "W_incorrect = np.array([[0.1, 2.0], [1.5, 0.3]])  # Weights for incorrect prediction\n",
    "b_incorrect = np.array([0.5, 0.6])  # Bias for incorrect prediction\n",
    "\n",
    "# Compute cost for correct predictions\n",
    "cost_correct = cost_softmax(X_correct, y_correct, W_correct, b_correct)\n",
    "\n",
    "# Compute cost for incorrect predictions\n",
    "cost_incorrect = cost_softmax(X_incorrect, y_incorrect, W_incorrect, b_incorrect)\n",
    "\n",
    "# Check if the cost for incorrect predictions is greater than for correct predictions\n",
    "assert cost_incorrect > cost_correct, f\"Test failed: Incorrect cost {cost_incorrect} is not greater than correct cost {cost_correct}\"\n",
    "\n",
    "# Print the costs for verification\n",
    "print(\"Cost for correct prediction:\", cost_correct)\n",
    "print(\"Cost for incorrect prediction:\", cost_incorrect)\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-YIb7zlveKq"
   },
   "source": [
    "### Computing Gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G3Vpn5bNvW3x"
   },
   "outputs": [],
   "source": [
    "def compute_gradient_softmax(X, y, W, b):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the cost function with respect to weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X.shape\n",
    "    z = np.dot(X,W) + b\n",
    "    y_pred = softmax(z)\n",
    "\n",
    "    grad_W = np.dot(X.T, (y_pred - y)) / n\n",
    "    grad_b = np.sum(y_pred - y,axis=0) / n\n",
    "    \n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S84yoIUx7vY7"
   },
   "source": [
    "### Test case for compute_gradient function:\n",
    "The test checks if the gradients from the function are close enough to the manually computed gradients using np.allclose, which accounts for potential floating-point discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-YSC_Ot70bZ",
    "outputId": "e87fbb1e-6048-419a-ae11-502fa5c0f044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. W: [[ 0.1031051   0.01805685 -0.12116196]\n",
      " [-0.13600547  0.00679023  0.12921524]]\n",
      "Gradient w.r.t. b: [-0.03290036  0.02484708  0.00805328]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple feature matrix and true labels\n",
    "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
    "y_test = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True labels (one-hot encoded, 3 classes)\n",
    "\n",
    "# Define weight matrix and bias vector\n",
    "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
    "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
    "\n",
    "# Compute the gradients using the function\n",
    "grad_W, grad_b = compute_gradient_softmax(X_test, y_test, W_test, b_test)\n",
    "\n",
    "# Manually compute the predicted probabilities (using softmax function)\n",
    "z_test = np.dot(X_test, W_test) + b_test\n",
    "y_pred_test = softmax(z_test)\n",
    "\n",
    "# Compute the manually computed gradients\n",
    "grad_W_manual = np.dot(X_test.T, (y_pred_test - y_test)) / X_test.shape[0]\n",
    "grad_b_manual = np.sum(y_pred_test - y_test, axis=0) / X_test.shape[0]\n",
    "\n",
    "# Assert that the gradients computed by the function match the manually computed gradients\n",
    "assert np.allclose(grad_W, grad_W_manual), f\"Test failed: Gradients w.r.t. W are not equal.\\nExpected: {grad_W_manual}\\nGot: {grad_W}\"\n",
    "assert np.allclose(grad_b, grad_b_manual), f\"Test failed: Gradients w.r.t. b are not equal.\\nExpected: {grad_b_manual}\\nGot: {grad_b}\"\n",
    "\n",
    "# Print the gradients for verification\n",
    "print(\"Gradient w.r.t. W:\", grad_W)\n",
    "print(\"Gradient w.r.t. b:\", grad_b)\n",
    "\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W75VL71ivpjG"
   },
   "source": [
    "### Implementing Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bbQ7SVw7vo-M"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "    alpha (float): Learning rate.\n",
    "    n_iter (int): Number of iterations.\n",
    "    show_cost (bool): Whether to display the cost at intervals.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Optimized weights, biases, and cost history.\n",
    "    \"\"\"\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients\n",
    "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
    "\n",
    "        W -= alpha * grad_W\n",
    "        b -= alpha * grad_B \n",
    "        \n",
    "    return W, b, cost_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBG9uSWKHDgX"
   },
   "source": [
    "## Preparing Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "prZ_zAvLpodE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_prepare_mnist(csv_file, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Reads the MNIST CSV file, splits data into train/test sets, and plots one image per class.\n",
    "\n",
    "    Arguments:\n",
    "    csv_file (str)       : Path to the CSV file containing MNIST data.\n",
    "    test_size (float)    : Proportion of the data to use as the test set (default: 0.2).\n",
    "    random_state (int)   : Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test : Split dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Separate labels and features\n",
    "    y = df.iloc[:, 0].values  # First column is the label\n",
    "    X = df.iloc[:, 1:].values  # Remaining columns are pixel values\n",
    "\n",
    "    # Normalize pixel values (optional but recommended)\n",
    "    X = X / 255.0  # Scale values between 0 and 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Plot one sample image per class\n",
    "    plot_sample_images(X, y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def plot_sample_images(X, y):\n",
    "    \"\"\"\n",
    "    Plots one sample image for each digit class (0-9).\n",
    "\n",
    "    Arguments:\n",
    "    X (np.ndarray): Feature matrix containing pixel values.\n",
    "    y (np.ndarray): Labels corresponding to images.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    unique_classes = np.unique(y)  # Get unique class labels\n",
    "\n",
    "    for i, digit in enumerate(unique_classes):\n",
    "        index = np.where(y == digit)[0][0]  # Find first occurrence of the class\n",
    "        image = X[index].reshape(28, 28)  # Reshape 1D array to 28x28\n",
    "\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Digit: {digit}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "ZtYR42Qas2uf",
    "outputId": "5e813934-8272-476e-eb15-f8c87a91572f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_file_path = \"./mnist_dataset.csv\"  # Path to saved dataset\n",
    "X_train, X_test, y_train, y_test = load_and_prepare_mnist(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyMBH4mQtzHA"
   },
   "source": [
    "### **A Quick debugging Step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIJhtnuCs7QF",
    "outputId": "994cc1d0-3098-4f53-95c9-8692811d519d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move forward: Dimension of Feture Matrix X and label vector y matched.\n"
     ]
    }
   ],
   "source": [
    "# Assert that X and y have matching lengths\n",
    "assert len(X_train) == len(y_train), f\"Error: X and y have different lengths! X={len(X_train)}, y={len(y_train)}\"\n",
    "print(\"Move forward: Dimension of Feture Matrix X and label vector y matched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TKIsKJcwFsv"
   },
   "source": [
    "## **Train the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEuTbCU0xAQW",
    "outputId": "832c9c89-12d8-40c5-9465-3477d424121b"
   },
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "J8e2mHmRv4fd",
    "outputId": "ede7dccf-93a2-4153-8c72-b6051b0007d1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Check if y_train is one-hot encoded\n",
    "if len(y_train.shape) == 1:\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output=False for newer versions of sklearn\n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))  # One-hot encode labels\n",
    "    y_test = encoder.transform(y_test.reshape(-1, 1))  # One-hot encode test labels\n",
    "\n",
    "# Now y_train is one-hot encoded, and we can proceed to use it\n",
    "d = X_train.shape[1]  # Number of features (columns in X_train)\n",
    "c = y_train.shape[1]  # Number of classes (columns in y_train after one-hot encoding)\n",
    "\n",
    "# Initialize weights with small random values and biases with zeros\n",
    "W = np.random.randn(d, c) * 0.01  # Small random weights initialized\n",
    "b = np.zeros(c)  # Bias initialized to 0\n",
    "\n",
    "# Set hyperparameters for gradient descent\n",
    "alpha = 0.1  # Learning rate\n",
    "n_iter = 1000  # Number of iterations to run gradient descent\n",
    "\n",
    "# Train the model using gradient descent\n",
    "W_opt, b_opt, cost_history = gradient_descent_softmax(X_train, y_train, W, b, alpha, n_iter, show_cost=True)\n",
    "\n",
    "# Plot the cost history to visualize the convergence\n",
    "plt.plot(cost_history)\n",
    "plt.title('Cost Function vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH4wNbhzys4f"
   },
   "source": [
    "## **Evaluating the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzV7BkRqOl5A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using confusion matrix, precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True labels\n",
    "    y_pred (numpy.ndarray): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    tuple: Confusion matrix, precision, recall, F1 score\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return cm, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uuGtvIlywK7J",
    "outputId": "1aa26874-665c-4d58-af5b-560f62c51d16"
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_test = predict_softmax(X_test, W_opt, b_opt)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels in numeric form\n",
    "\n",
    "# Evaluate the model\n",
    "cm, precision, recall, f1 = evaluate_classification(y_test_labels, y_pred_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "cax = ax.imshow(cm, cmap='Blues')  # Use a color map for better visualization\n",
    "\n",
    "# Dynamic number of classes\n",
    "num_classes = cm.shape[0]\n",
    "ax.set_xticks(range(num_classes))\n",
    "ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([f'Predicted {i}' for i in range(num_classes)])\n",
    "ax.set_yticklabels([f'Actual {i}' for i in range(num_classes)])\n",
    "\n",
    "# Add labels to each cell in the confusion matrix\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i, j] > np.max(cm) / 2 else 'black')\n",
    "\n",
    "# Add grid lines and axis labels\n",
    "ax.grid(False)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv8J5hNCPzl6"
   },
   "source": [
    "# Linear Seperability and Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTyaPcM-P69S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1uPYyhotoAf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prediction Function:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
